{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Decoder Architecture**\n",
        "\n",
        "The decoder takes as input the hidden states generated by the encoder and the previously generated output tokens and uses them to predict the next output token. At each step, the decoder attends to different parts of the input sequence using its attention mechanism, allowing it to capture complex relationships between the input and output sequences."
      ],
      "metadata": {
        "id": "iQERvwYHfJLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://media.geeksforgeeks.org/wp-content/uploads/20240110165738/Transformer-python.webp)"
      ],
      "metadata": {
        "id": "PaJmwTQjfKIj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3Uya026KaqXY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 30\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048\n",
        "num_layers = 5"
      ],
      "metadata": {
        "id": "M5zMA2xbayaO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def scaled_attention(q, k, v, mask):                  # 30 x 8 x 200 x 64\n",
        "  shape = max_sequence_length                         # 200\n",
        "  d_k = q.size()[-1]                                  # 1 x 1\n",
        "  scaled = (q @ k.transpose(-2, -1)) / math.sqrt(d_k) # 30 x 8 x 200 x 200\n",
        "\n",
        "  if mask:\n",
        "    tril = torch.tril(torch.ones(shape, shape))       # 200 x 200\n",
        "    mask = tril.masked_fill(tril == 0, float('-inf')) # 200 x 200\n",
        "    mask = mask.masked_fill(tril == 1, 0)             # 200 x 200\n",
        "    scaled += mask                                    # 30 x 8 x 200 x 200\n",
        "\n",
        "  attention = F.softmax(scaled, dim = -1)             # 30 x 8 x 200 x 200\n",
        "  values = attention @ v                              # 30 x 8 x 200 x 64\n",
        "\n",
        "  return values"
      ],
      "metadata": {
        "id": "Sup7sLMji6nA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model                           # 512\n",
        "    self.num_heads = num_heads                       # 8\n",
        "    self.head_dim = d_model // num_heads             # 64\n",
        "    self.qkv_layer = nn.Linear(d_model, 3 * d_model) # 512 x 1536\n",
        "    self.lin_layer = nn.Linear(d_model, d_model)     # 512 x 512\n",
        "\n",
        "  def forward(self, x, mask = False):\n",
        "    batch_size, sequence_len, input_dim = x.size()   # 30 x 200 x 512\n",
        "    qkv = self.qkv_layer(x)                          # 30 x 200 x 1536\n",
        "    qkv = qkv.reshape(batch_size, sequence_len,\n",
        "                self.num_heads, 3 * self.head_dim)   # 30 x 200 x 8 x 196\n",
        "    qkv = qkv.permute(0, 2, 1, 3)                    # 30 x 8 x 200 x 196\n",
        "    q, k, v = qkv.chunk(3, dim = -1)                 # (30 x 8 x 200 x 64) * 3\n",
        "    values = scaled_attention(q, k, v, mask)         # 30 x 8 x 200 x 64\n",
        "    values = values.reshape(batch_size,sequence_len,\n",
        "                    self.num_heads * self.head_dim)  # 30 x 200 x 512\n",
        "    out = self.lin_layer(values)                     # 30 x 200 x 512\n",
        "    return out"
      ],
      "metadata": {
        "id": "O5z9lKGha4JJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadCrossAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model                           # 512\n",
        "    self.num_heads = num_heads                       # 8\n",
        "    self.head_dim = d_model // num_heads             # 64\n",
        "    self.kv_layer = nn.Linear(d_model, 2 * d_model)  # 512 x 1024\n",
        "    self.q_layer = nn.Linear(d_model, d_model)       # 512 x 512\n",
        "    self.lin_layer = nn.Linear(d_model, d_model)     # 512 x 512\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    batch_size, sequence_len, input_dim = x.size()   # 30 x 200 x 512\n",
        "    kv = self.kv_layer(x)                            # 30 x 200 x 1024\n",
        "    q = self.q_layer(y)                              # 30 x 200 x 512\n",
        "    kv = kv.reshape(batch_size, sequence_len,\n",
        "                self.num_heads, 2 * self.head_dim)   # 30 x 200 x 8 x 128\n",
        "    q = q.reshape(batch_size, sequence_len,\n",
        "                self.num_heads, self.head_dim)       # 30 x 200 x 8 x 64\n",
        "    kv = kv.permute(0, 2, 1, 3)                      # 30 x 8 x 200 x 128\n",
        "    q = q.permute(0, 2, 1, 3)                        # 30 x 8 x 200 x 64\n",
        "    k, v = kv.chunk(2, dim = -1)                     # (30 x 8 x 200 x 64) * 2\n",
        "    values = scaled_attention(q, k, v, mask = False) # 30 x 8 x 200 x 64\n",
        "    values = values.reshape(batch_size,sequence_len,\n",
        "                    self.num_heads * self.head_dim)  # 30 x 200 x 512\n",
        "    out = self.lin_layer(values)                     # 30 x 200 x 512\n",
        "    return out"
      ],
      "metadata": {
        "id": "GmLRYp3YuOMB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, hidden, prob):\n",
        "    super(PositionwiseFeedForward, self).__init__()\n",
        "    self.linear1 = nn.Linear(d_model, hidden)       # 512, 2048\n",
        "    self.linear2 = nn.Linear(hidden, d_model)       # 2048 x 512\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(prob)\n",
        "\n",
        "  def forward(self, x):                             # 30 x 200 x 512\n",
        "    x = self.linear1(x)                             # 30 x 200 x 2048\n",
        "    x = self.relu(x)                                # 30 x 200 x 2048\n",
        "    x = self.dropout(x)                             # 30 x 200 x 2048\n",
        "    x = self.linear2(x)                             # 30 x 200 x 512\n",
        "    return x"
      ],
      "metadata": {
        "id": "seAs_Ai8yycO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, params_shape, eps = 1e-5):\n",
        "    super().__init__()\n",
        "    self.params_shape = params_shape                         # 1 x 512\n",
        "    self.eps = eps\n",
        "    self.gamma = nn.Parameter(torch.ones(params_shape))      # 1 x 512\n",
        "    self.beta = nn.Parameter(torch.zeros(params_shape))      # 1 x 512\n",
        "\n",
        "  def forward(self, input):\n",
        "    dims = [-(i + 1) for i in range(len(self.params_shape))] # 1 x params_shape\n",
        "    mean = input.mean(dim = dims, keepdim = True)            # 30 x 200 x 1\n",
        "    var = (((input - mean) ** 2)\n",
        "                .mean(dim = dims , keepdim = True))          # 30 x 200 x 1\n",
        "    sd = (var + self.eps).sqrt()                             # 30 x 200 x 1\n",
        "    X_dash = (input - mean) / sd                             # 30 x 200 x 512\n",
        "    Y = self.gamma * X_dash + self.beta                      # 30 x 200 x 512\n",
        "\n",
        "    return Y"
      ],
      "metadata": {
        "id": "qahhdhM7oqIm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, drop_prob, ffn_hidden):\n",
        "    super().__init__()\n",
        "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "    self.norm1 = nn.LayerNorm([d_model])\n",
        "    self.dropout1 = nn.Dropout(drop_prob)\n",
        "    self.enc_dec_attn = MultiHeadCrossAttention(d_model, num_heads)\n",
        "    self.norm2 = nn.LayerNorm([d_model])\n",
        "    self.dropout2 = nn.Dropout(drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model, ffn_hidden, drop_prob)\n",
        "    self.norm3 = nn.LayerNorm([d_model])\n",
        "    self.dropout3 = nn.Dropout(drop_prob)\n",
        "\n",
        "  def forward(self, x, y, mask):\n",
        "    res_y = y                            # 30 x 200 x 512\n",
        "    y = self.self_attn(y, mask = True)   # 30 x 200 x 512\n",
        "    y = self.dropout1(y)                 # 30 x 200 x 512\n",
        "    y = self.norm1(y + res_y)            # 30 x 200 x 512\n",
        "\n",
        "    res_y = y                            # 30 x 200 x 512\n",
        "    y = self.enc_dec_attn(x, y)          # 30 x 200 x 512\n",
        "    y = self.dropout2(y)                 # 30 x 200 x 512\n",
        "    y = self.norm2(y + res_y)            # 30 x 200 x 512\n",
        "\n",
        "    res_y = y                            # 30 x 200 x 512\n",
        "    y = self.ffn(y)                      # 30 x 200 x 512\n",
        "    y = self.dropout3(y)                 # 30 x 200 x 512\n",
        "    y = self.norm3(y + res_y)            # 30 x 200 x 512\n",
        "\n",
        "    return y\n"
      ],
      "metadata": {
        "id": "vxzSRRSta4Lq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialDecoder(nn.Sequential):\n",
        "  def forward(self, *inputs):\n",
        "    x, y, mask = inputs\n",
        "    for module in self._modules.values():\n",
        "        y = module(x, y, mask) #30 x 200 x 512\n",
        "    return y"
      ],
      "metadata": {
        "id": "pwM4mLSCzE8y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model, num_layers, num_heads, drop_prob, ffn_hidden):\n",
        "    super().__init__()\n",
        "    self.layers = SequentialDecoder(*[DecoderLayer(d_model, num_heads, drop_prob, ffn_hidden) for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self, x, y, mask):\n",
        "    # x : Eng : 30 x 200 x 512\n",
        "    # y : Eng : 30 x 200 x 512\n",
        "    # mask : 200 x 200\n",
        "    y = self.layers(x, y, mask)  # 30 x  200 x 512\n",
        "    return y"
      ],
      "metadata": {
        "id": "vG0s-vtGa4Nw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(batch_size, max_sequence_length, d_model)\n",
        "y = torch.randn(batch_size, max_sequence_length, d_model)\n",
        "mask = torch.randn(max_sequence_length, max_sequence_length)\n",
        "decoder = Decoder(d_model, num_layers, num_heads, drop_prob, ffn_hidden)\n",
        "\n",
        "y = decoder(x,y,mask)\n"
      ],
      "metadata": {
        "id": "0QrLVFO3zw2w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0][0][:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiYB1Qix93-A",
        "outputId": "192c13ee-7d35-4eed-9928-be574e872076"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.3718, -0.3440, -0.9321,  0.7372, -1.7299, -0.8228, -0.8395,  1.3644,\n",
              "        -0.0908, -0.5668,  0.4527,  0.3867,  0.0058, -0.3630,  0.5339,  0.8531,\n",
              "        -0.3019,  1.2913,  2.8505,  0.4399])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0][0][:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxEfwPfl-MNT",
        "outputId": "c1340522-7714-488a-c4b7-6724345e83a5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.4367, -1.8011,  1.0248,  1.5188,  0.6213,  0.5930,  0.6552, -1.9216,\n",
              "         0.7310, -0.7416,  0.5891, -1.0531,  1.1018,  0.5524,  1.6332,  0.5605,\n",
              "        -0.6998, -0.1624,  1.2620,  0.1964], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}