{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SELF-ATTENTION MECHANISM**\n",
        "\n",
        "An attention mechanism is an Encoder-Decoder kind of neural network architecture that allows the model to focus on specific sections of the input while executing a task. It dynamically assigns weights to different elements in the input, indicating their relative importance or relevance. By incorporating attention, the model can selectively attend to and process the most relevant information, capturing dependencies and relationships within the data. This mechanism is particularly valuable in tasks involving sequential or structured data, such as natural language processing or computer vision, as it enables the model to effectively handle long-range dependencies and improve performance by selectively attending to important features or contexts."
      ],
      "metadata": {
        "id": "0lVQF38bNAT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n"
      ],
      "metadata": {
        "id": "tPvLibtpOmnR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L, dk, dv = 4, 8, 8\n",
        "\n",
        "q = np.random.randn(L, dk)\n",
        "k = np.random.randn(L, dk)\n",
        "v = np.random.randn(L, dv)"
      ],
      "metadata": {
        "id": "ZWBTLHwHXecS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Self attention**\n",
        "\n",
        "```\n",
        "Q = What I want\n",
        "K = What I have\n",
        "V = What I contribute\n",
        "```\n",
        "\n",
        "![attention](https://media.geeksforgeeks.org/wp-content/uploads/20240110170625/Scaled-Dot-Product-and-Multi-Head-Attentions.webp)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HFdw_DqaYGwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mmul = np.matmul(q, k.T)"
      ],
      "metadata": {
        "id": "H4eGKdYhYWnF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.var(), v.var(), mmul.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgTyO0J6YADS",
        "outputId": "6bf57c1d-1997-4a56-bd86-54fda5df61dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5302327041201682, 1.0245439008600614, 2.5938049950396618)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mmul /= math.sqrt(dk)\n",
        "mmul.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB2JJlBqYd3m",
        "outputId": "e8ad466e-2e03-44b1-be80-cb093528ad41"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3242256243799577"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Attention with Masking**\n",
        "\n",
        "**DECODER**\n",
        "\n",
        "We need each token to communicate only to the past not the future tokens\n",
        "\n",
        "For now, for every single batch element independently, for each T'th token in that sequence, we will calculate the average of all the vectors of this and the previous tokens"
      ],
      "metadata": {
        "id": "8di3mzIdZrGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "ZDrn2sjHaQiK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method - 1 For loops**"
      ],
      "metadata": {
        "id": "7kRZIrVsY89f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 1, 4, 4\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvEb-y8-Yh6d",
        "outputId": "8aa5428f-1031-45d5-d029-8cd29b39e644"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention = torch.zeros((B, T, C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b, :t+1] # (t, C)\n",
        "        attention[b, t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "HEy5cErOZOPu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x.shape, attention.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArBB24LFZOSi",
        "outputId": "1aa58bbb-44b6-4b58-a13f-6fafbcaa9a59"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 4]), torch.Size([1, 4, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgI81V5La4Sk",
        "outputId": "f20ef544-568e-4895-d0ea-3c7f08719c09"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152],\n",
              "         [ 0.4033, -0.0222,  0.2974, -0.4254],\n",
              "         [ 0.3892,  0.3745, -0.2517, -0.4537],\n",
              "         [ 0.3509,  0.2209, -0.4190,  0.0456]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the above process is computationally heavy\n",
        "\n",
        "For that, we could use a shortcut via matrix multiplication\n",
        "- tril for generating a lower triangular matrix\n",
        "- normalise each row such that sum(row) = 1\n",
        "- matrix multiply this tri_matrix and the x to get xbow"
      ],
      "metadata": {
        "id": "uWhhhTE8ZTSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method - 2 Matrix multiplication**"
      ],
      "metadata": {
        "id": "UxXFNAZ2ZXcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.ones(4, 4)\n",
        "b = torch.randint(0, 10, (4, 4)).float()\n",
        "c = a @ b\n",
        "print(f'a => {a}\\n----\\nb => {b}\\n----\\nc => {c}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6nuo5VqZOU5",
        "outputId": "094ebafb-a753-4f56-cb52-41275bd4bf26"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a => tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "----\n",
            "b => tensor([[1., 4., 9., 5.],\n",
            "        [3., 6., 2., 0.],\n",
            "        [2., 1., 6., 5.],\n",
            "        [9., 4., 5., 9.]])\n",
            "----\n",
            "c => tensor([[15., 15., 22., 19.],\n",
            "        [15., 15., 22., 19.],\n",
            "        [15., 15., 22., 19.],\n",
            "        [15., 15., 22., 19.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This gives us the sum of the previous context for each of the tokens\n",
        "torch.manual_seed(35)\n",
        "\n",
        "a = torch.tril(torch.ones(4, 4)) # tril gives us the lower triangular matrix\n",
        "b = torch.randint(0, 10, (4, 4)).float()\n",
        "\n",
        "c = a @ b\n",
        "print(f'a => {a}\\n----\\nb => {b}\\n----\\nc => {c}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29CEO_CRZOW-",
        "outputId": "db02f789-2abd-42f4-9fb6-a54c8c39c208"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a => tensor([[1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1.]])\n",
            "----\n",
            "b => tensor([[7., 9., 8., 7.],\n",
            "        [1., 8., 7., 6.],\n",
            "        [3., 8., 5., 3.],\n",
            "        [6., 2., 7., 3.]])\n",
            "----\n",
            "c => tensor([[ 7.,  9.,  8.,  7.],\n",
            "        [ 8., 17., 15., 13.],\n",
            "        [11., 25., 20., 16.],\n",
            "        [17., 27., 27., 19.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can even get the average of the previous context for each of the tokens by normalising\n",
        "torch.manual_seed(35)\n",
        "\n",
        "a = torch.tril(torch.ones(4, 4))\n",
        "a = a / torch.sum(a, 1, keepdim = True)\n",
        "b = torch.randint(0, 10, (4, 4)).float()\n",
        "\n",
        "c = a @ b\n",
        "print(f'a => {a}\\n----\\nb => {b}\\n----\\nc => {c}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0BENQAHZd04",
        "outputId": "55a78d67-2ef8-4906-b5f3-3febbb6918bb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a => tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500]])\n",
            "----\n",
            "b => tensor([[7., 9., 8., 7.],\n",
            "        [1., 8., 7., 6.],\n",
            "        [3., 8., 5., 3.],\n",
            "        [6., 2., 7., 3.]])\n",
            "----\n",
            "c => tensor([[7.0000, 9.0000, 8.0000, 7.0000],\n",
            "        [4.0000, 8.5000, 7.5000, 6.5000],\n",
            "        [3.6667, 8.3333, 6.6667, 5.3333],\n",
            "        [4.2500, 6.7500, 6.7500, 4.7500]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 1, 4, 4\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "ws = torch.tril(torch.ones(T, T))\n",
        "ws = ws / torch.sum(ws, 1, keepdim = True)\n",
        "\n",
        "attention = ws @ x\n",
        "\n",
        "(x, attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1eYIcIcZd3K",
        "outputId": "24fcc16a-6275-43b4-c260-4ba0164c0315"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152],\n",
              "          [ 0.6258,  0.0255,  0.9545,  0.0643],\n",
              "          [ 0.3612,  1.1679, -1.3499, -0.5102],\n",
              "          [ 0.2360, -0.2398, -0.9211,  1.5433]]]),\n",
              " tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152],\n",
              "          [ 0.4033, -0.0222,  0.2974, -0.4254],\n",
              "          [ 0.3892,  0.3745, -0.2517, -0.4537],\n",
              "          [ 0.3509,  0.2209, -0.4190,  0.0456]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Method - 3 Using softmax**"
      ],
      "metadata": {
        "id": "bkMyMIMPZkzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 1, 4, 4\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "w = tril.masked_fill(tril == 0, float('-inf'))\n",
        "\n",
        "sm = F.softmax(w, dim = -1)\n",
        "attention = sm @ x\n",
        "\n",
        "(x[0], attention[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7V-yCN0Zd5P",
        "outputId": "cdb26b80-cb8c-48f6-8ea4-7bed82a83673"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1808, -0.0700, -0.3596, -0.9152],\n",
              "         [ 0.6258,  0.0255,  0.9545,  0.0643],\n",
              "         [ 0.3612,  1.1679, -1.3499, -0.5102],\n",
              "         [ 0.2360, -0.2398, -0.9211,  1.5433]]),\n",
              " tensor([[ 0.1808, -0.0700, -0.3596, -0.9152],\n",
              "         [ 0.4033, -0.0222,  0.2974, -0.4254],\n",
              "         [ 0.3892,  0.3745, -0.2517, -0.4537],\n",
              "         [ 0.3509,  0.2209, -0.4190,  0.0456]]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis = 1)).T"
      ],
      "metadata": {
        "id": "dc5NhZSGfmed"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.tril(np.ones((4, 4)))\n",
        "mask[mask == 0] = -np.inf\n",
        "mask[mask == 1] = 0\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iHlRDkxfz62",
        "outputId": "c049c7a7-8325-4ea6-fad5-859a1680a4fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0., -inf, -inf, -inf],\n",
              "       [  0.,   0., -inf, -inf],\n",
              "       [  0.,   0.,   0., -inf],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self attention**\n",
        "\n",
        "In the above, we have created the tril in a normal distribution, but we want the tokens to be in a context specific way, and we find those affinities by using a dot product between the query of the current token and the keys of the previous tokens and then matrix multiply with the value matrix to get a more context aware new_V\n",
        "\n",
        "Attention(Q,K,V) = ${\\frac{Q.K.trans}{\\sqrt{dk}}}.V$"
      ],
      "metadata": {
        "id": "QS6ovbMFaFA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention = softmax(mmul + mask)\n",
        "attention\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oERFoIrTaF-p",
        "outputId": "599241ae-b1fc-4861-8a34-fc62e364ad20"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , 0.        ],\n",
              "       [0.68380936, 0.31619064, 0.        , 0.        ],\n",
              "       [0.27508722, 0.06704027, 0.65787251, 0.        ],\n",
              "       [0.34092999, 0.17502176, 0.18444345, 0.2996048 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_v = np.matmul(attention, v)\n",
        "new_v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypYqTk1mhNc4",
        "outputId": "b34970cf-a020-4769-e7a3-528efe5a29c5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.86513062, -1.43718951,  0.35290122,  1.08955526,  0.88640486,\n",
              "         0.6905449 ,  1.04516679,  0.05863759],\n",
              "       [ 1.12439035, -0.91937306,  0.42884466,  0.91799703,  0.62062161,\n",
              "         1.03561574,  0.38338553, -1.03196514],\n",
              "       [ 0.82566275, -0.11224609,  0.88871722, -0.01261991,  0.87309642,\n",
              "         0.83774284, -0.43442011,  0.39228697],\n",
              "       [ 0.78805628, -0.01149165,  0.59020333,  0.07813057,  0.71829653,\n",
              "         0.99530663, -0.10141863, -0.34828239]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bxKnryXhWq_",
        "outputId": "95d7413e-79e3-492d-9e83-84806256ad6a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.86513062, -1.43718951,  0.35290122,  1.08955526,  0.88640486,\n",
              "         0.6905449 ,  1.04516679,  0.05863759],\n",
              "       [ 1.68507811,  0.20048216,  0.59308367,  0.54697681,  0.04582575,\n",
              "         1.78188282, -1.04781502, -3.39055599],\n",
              "       [ 0.72158097,  0.40990617,  1.14289334, -0.53051633,  0.95183428,\n",
              "         0.80308091, -0.99059668,  0.91729068],\n",
              "       [ 0.21725647,  1.22760415,  0.51830606, -0.97199399,  0.77607043,\n",
              "         1.00094486, -0.30589572,  0.18677643]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SUMMARY**"
      ],
      "metadata": {
        "id": "J0a1Cm5YjrMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return (np.exp(x).T / np.sum(np.exp(x), axis = 1)).T\n",
        "\n",
        "def scaled_attention(q, k, v, mask = None):\n",
        "  dk = q.shape[-1]\n",
        "  mmul = np.matmul(q, k.T) / np.sqrt(dk)\n",
        "\n",
        "  if mask is not None:\n",
        "    # Decoder\n",
        "    mmul = mmul + mask\n",
        "\n",
        "  attention = softmax(mmul)\n",
        "  new_v = np.matmul(attention, v)\n",
        "\n",
        "  return new_v"
      ],
      "metadata": {
        "id": "rKzNxkStiIur"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = np.random.randn(L, dk)\n",
        "k = np.random.randn(L, dk)\n",
        "v = np.random.randn(L, dv)\n",
        "\n",
        "mask = np.tril(np.ones((4, 4)))\n",
        "mask[mask == 0] = -np.inf\n",
        "mask[mask == 1] = 0"
      ],
      "metadata": {
        "id": "0bZJyxKEjNyf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_attention(q, k, v, mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68q9x-Rhjm45",
        "outputId": "70171610-d571-46c4-8f1c-5b0452a91c1c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.81560115,  0.17641806, -0.02343507,  0.81003228,  0.11657641,\n",
              "        -0.7274329 ,  0.46035725, -1.25286399],\n",
              "       [ 0.19743813, -0.2320181 ,  0.18984669,  1.06292392,  0.41770658,\n",
              "         0.25696126,  0.81283491, -0.76352402],\n",
              "       [ 0.29775435, -0.27722671,  0.16728917,  0.96637163,  0.36491109,\n",
              "         0.10536374,  0.64879437, -0.79355619],\n",
              "       [-0.11198217, -0.03530606,  0.02255145,  0.96962907, -0.39601461,\n",
              "         1.11035952, -0.02437963, -0.60231577]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}
