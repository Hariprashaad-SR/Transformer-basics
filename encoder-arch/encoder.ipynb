{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder Architecture**"
      ],
      "metadata": {
        "id": "Fwt3xnYR53GS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://media.geeksforgeeks.org/wp-content/uploads/20240110165738/Transformer-python.webp)"
      ],
      "metadata": {
        "id": "BoGKf4LhIFKt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HGJkRgULba_N"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 30\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048\n",
        "num_layers = 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "qEYSg901xAZj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def scaled_attention(q, k, v, mask):                  # 30 x 8 x 200 x 64\n",
        "  shape = max_sequence_length                         # 200 x 200\n",
        "  d_k = q.size()[-1]                                  # 1 x 1\n",
        "  scaled = (q @ k.transpose(-2, -1)) / math.sqrt(d_k) # 30 x 8 x 200 x 200\n",
        "\n",
        "  if mask:\n",
        "    tril = torch.tril(torch.ones(shape, shape))       # 200 x 200\n",
        "    mask = tril.masked_fill(tril == 0, float('-inf')) # 200 x 200\n",
        "    mask = mask.masked_fill(tril == 1, 0)             # 200 x 200\n",
        "    scaled += mask                                    # 30 x 8 x 200 x 200\n",
        "\n",
        "  attention = F.softmax(scaled, dim = -1)             # 30 x 8 x 200 x 200\n",
        "  values = attention @ v                              # 30 x 8 x 200 x 64\n",
        "\n",
        "  return values"
      ],
      "metadata": {
        "id": "oY-QTCC2JLh1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model                           # 512\n",
        "    self.num_heads = num_heads                       # 8\n",
        "    self.head_dim = d_model // num_heads             # 64\n",
        "    self.qkv_layer = nn.Linear(d_model, 3 * d_model) # 512 x 1536\n",
        "    self.lin_layer = nn.Linear(d_model, d_model)     # 512 x 512\n",
        "\n",
        "  def forward(self, x, mask = False):\n",
        "    batch_size, sequence_len, input_dim = x.size()   # 30 x 200 x 512\n",
        "    qkv = self.qkv_layer(x)                          # 30 x 200 x 1536\n",
        "    qkv = qkv.reshape(batch_size, sequence_len,\n",
        "                self.num_heads, 3 * self.head_dim)   # 30 x 200 x 8 x 196\n",
        "    qkv = qkv.permute(0, 2, 1, 3)                    # 30 x 8 x 200 x 196\n",
        "    q, k, v = qkv.chunk(3, dim = -1)                 # (30 x 8 x 200 x 64) * 3\n",
        "    values = scaled_attention(q, k, v, mask)         # 30 x 8 x 200 x 64\n",
        "    values = values.reshape(batch_size,sequence_len,\n",
        "                    self.num_heads * self.head_dim)  # 30 x 200 x 512\n",
        "    out = self.lin_layer(values)                     # 30 x 200 x 512\n",
        "    return out"
      ],
      "metadata": {
        "id": "bZoeT4KaIkdz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, params_shape, eps = 1e-5):\n",
        "    super().__init__()\n",
        "    self.params_shape = params_shape                         # 1 x 512\n",
        "    self.eps = eps\n",
        "    self.gamma = nn.Parameter(torch.ones(params_shape))      # 1 x 512\n",
        "    self.beta = nn.Parameter(torch.zeros(params_shape))      # 1 x 512\n",
        "\n",
        "  def forward(self, input):\n",
        "    dims = [-(i + 1) for i in range(len(self.params_shape))] # 1 x params_shape\n",
        "    mean = input.mean(dim = dims, keepdim = True)            # 30 x 200 x 1\n",
        "    var = (((input - mean) ** 2)\n",
        "                .mean(dim = dims , keepdim = True))          # 30 x 200 x 1\n",
        "    sd = (var + self.eps).sqrt()                             # 30 x 200 x 1\n",
        "    X_dash = (input - mean) / sd                             # 30 x 200 x 512\n",
        "    Y = self.gamma * X_dash + self.beta                      # 30 x 200 x 512\n",
        "\n",
        "    return Y"
      ],
      "metadata": {
        "id": "2vcT0A_hi-e7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, hidden, prob):\n",
        "    super(PositionwiseFeedForward, self).__init__()\n",
        "    self.linear1 = nn.Linear(d_model, hidden)       # 512, 2048\n",
        "    self.linear2 = nn.Linear(hidden, d_model)       # 2048 x 512\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(prob)\n",
        "\n",
        "  def forward(self, x):                             # 30 x 200 x 512\n",
        "    x = self.linear1(x)                             # 30 x 200 x 2048\n",
        "    x = self.relu(x)                                # 30 x 200 x 2048\n",
        "    x = self.dropout(x)                             # 30 x 200 x 2048\n",
        "    x = self.linear2(x)                             # 30 x 200 x 512\n",
        "    return x"
      ],
      "metadata": {
        "id": "FFiXrhV8dDEH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, drop_prob, ffn_hidden):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.attention = MultiheadAttention(d_model, num_heads)\n",
        "    self.norm1 = LayerNorm([d_model])\n",
        "    self.dropout1 = nn.Dropout(drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model, ffn_hidden, drop_prob)\n",
        "    self.dropout2 = nn.Dropout(drop_prob)\n",
        "    self.norm2 = LayerNorm([d_model])\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual_x = x                  # 30 x 200 x 512\n",
        "    x = self.attention(x)           # 30 x 200 x 512\n",
        "    x = self.dropout1(x)            # 30 x 200 x 512\n",
        "    x = self.norm1(x + residual_x)  # 30 x 200 x 512\n",
        "    residual_x = x                  # 30 x 200 x 512\n",
        "    x = self.ffn(x)                 # 30 x 200 x 512\n",
        "    x = self.dropout2(x)            # 30 x 200 x 512\n",
        "    x = self.norm2(x + residual_x)  # 30 x 200 x 512\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "SeOsQ9Riyp0d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_model, num_layers, num_heads, drop_prob, ffn_hidden):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(*[EncoderLayer(d_model, num_heads, drop_prob, ffn_hidden) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "1-LuT1g4ip31"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(batch_size, max_sequence_length, d_model)\n",
        "encoder = Encoder(d_model, num_layers, num_heads, drop_prob, ffn_hidden)\n",
        "y = encoder(x)\n"
      ],
      "metadata": {
        "id": "_xyPtLHy0voa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6DHk0Q65lbe",
        "outputId": "c1b473b3-5322-4230-9d81-f87abfd324ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([30, 200, 512]), torch.Size([30, 200, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0][0][:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IchETtoV05Fl",
        "outputId": "35fdc5e4-0080-45ac-fd98-1b81cd674b05"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.4521, -0.8794,  1.7164, -1.2958, -0.2280, -0.0445,  1.3175,  0.9746,\n",
              "        -0.0713, -2.6337, -0.6067,  1.6901, -1.1586,  0.2260, -0.4585, -0.1086,\n",
              "         1.8267,  2.2390, -0.7567, -1.0699])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0][0][:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlnbO8lP5otZ",
        "outputId": "103926cd-9bd1-4ef0-a22a-80795c9578d9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.5077, -0.0504,  1.5386, -1.8080, -0.1344, -0.2638,  1.2649,  0.9424,\n",
              "        -0.0458, -2.0284, -0.9581,  1.4612, -1.0095, -0.2887, -0.0793,  0.4107,\n",
              "         1.6133,  1.6992, -0.3484, -1.3440], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}