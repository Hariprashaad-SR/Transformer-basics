{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **TRANSFORMER**\n",
        "\n",
        "```\n",
        "1. input data\n",
        "\n",
        "2. ohe = one hot encoding (input_size x vocab_size)\n",
        "\n",
        "3. x(input_size x nemb) = ohe(input_size x vocab_size) @ we(vocab_size x nemb)\n",
        "\n",
        "4. xd = x + pos_emb\n",
        "\n",
        "5. wq, wk, wv = (nemb x nemb)\n",
        "\n",
        "6. xd @ wq,wk,wv = 3 * (input_size x nemb)\n",
        "```"
      ],
      "metadata": {
        "id": "Uqj8SxCbE132"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **POSITIONAL ENCODING**\n",
        "\n",
        "Positional encodings are crucial in Transformer models for several reasons:\n",
        "\n",
        "- Preserving Sequence Order: Transformer models process tokens in parallel, lacking inherent knowledge of token order. Positional encodings provide the model with information about the position of tokens in the sequence, ensuring that the model can differentiate between tokens based on their position. This is essential for tasks where word order matters, such as language translation and text generation.\n",
        "\n",
        "- Maintaining Contextual Information: In natural language processing tasks, the meaning of a word often depends on its position in the sentence. For example, in the sentence “The cat sat on the mat,” the word “cat” has a different meaning than in “The mat sat on the cat.” transformer\n",
        "\n",
        "- Enhancing Generalization: By incorporating positional information, transformer models can generalize better across sequences of different lengths. This is particularly important for tasks where the length of the input sequence varies, such as document summarization or question answering. Positional encodings enable the model to handle input sequences of varying lengths without sacrificing performance.\n",
        "\n",
        "- Mitigating Symmetry: Without positional encodings, the self-attention mechanism in Transformer models would treat tokens symmetrically, potentially leading to ambiguous representations. Positional encodings introduce an asymmetry into the model, ensuring that tokens at different positions are treated differently, thereby improving the model’s ability to capture long-range dependencies.\n",
        "\n",
        "In summary, positional encodings are essential in Transformer models for preserving sequence order, maintaining contextual information, enhancing generalization, and mitigating symmetry. They enable Transformer models to effectively process and understand input sequences, leading to improved performance across a wide range of natural language processing tasks.\n",
        "\n",
        "---\n",
        "\n",
        "positional encodin$g_{(pos,2i)}$ => $sin({\\frac{pos}{10000^{2_{i} / d_{model}}}})$\n",
        "\n",
        "positional encodin$g_{(pos, 2i + 1)}$ => $cos({\\frac{pos}{10000^{2_i - 1 / d_{model}}}})$\n",
        "\n",
        "```\n",
        "i => dimension index\n",
        "dmodel => embedding length\n",
        "pos => position of word in sequence\n",
        "\n",
        "Reasons:\n",
        "- Periodicity\n",
        "- Constrained values\n",
        "- Easy to extrapolate for long sequences\n",
        "```\n",
        "```\n",
        "msl = x\n",
        "dm = y\n",
        "\n",
        "even_i.shape = odd_i.shape = (y // 2)\n",
        "denom.shape = (y//2, 1)\n",
        "pos.shape = (x, 1)\n",
        "even_PE.shape = odd_PE.shape = (x, y//2)\n",
        "stacked.shape = (x, y//2, 2)\n",
        "PE.shape = (x, y)\n",
        "```"
      ],
      "metadata": {
        "id": "V0o5UB4KKLu7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_szdbwoZ6ALN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constants\n",
        "\n",
        "max_seq_len = 10\n",
        "d_model = 6"
      ],
      "metadata": {
        "id": "l7myVc2iEksY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the even and odd indices\n",
        "\n",
        "even_i = torch.arange(0, d_model, 2).float()\n",
        "odd_i = torch.arange(1, d_model, 2).float()\n",
        "\n",
        "(even_i.shape, odd_i.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO_78GT_G57x",
        "outputId": "e1eadd73-b893-401e-eecc-a9ca7526241f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3]), torch.Size([3]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the denominator\n",
        "\n",
        "even_denom = pow(10000, even_i / d_model)\n",
        "odd_denom = pow(10000, (odd_i - 1) / d_model)\n",
        "\n",
        "even_denom == odd_denom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBz7vAO7HOyb",
        "outputId": "bf2a6567-ac38-4824-fc63-1c67e5d92468"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "denom = even_denom\n",
        "denom.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c109Q5xH_0a",
        "outputId": "7d8bf28b-0510-4e09-90cc-6d6132e2889e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# positions of the input data\n",
        "\n",
        "position = torch.arange(max_seq_len, dtype = torch.float).reshape(max_seq_len, 1)\n",
        "position.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlfPN9l8IFwy",
        "outputId": "fd1c483f-a218-4bea-81dc-c35edd7a04e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(position/denom).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BUDfyTGSXAm",
        "outputId": "445d7625-e731-45d6-e30d-0424101c38fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# even positional encoding\n",
        "\n",
        "even_PE = torch.sin(position / denom)\n",
        "even_PE.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7UnApa0Hd2J",
        "outputId": "c48f7317-cedf-4355-eec8-c09d61c41506"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# odd positional encoding\n",
        "\n",
        "odd_PE = torch.cos(position / denom)\n",
        "odd_PE.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf6cHi0-IVcs",
        "outputId": "8d658c30-4736-4f96-dba1-88a79aadaee8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combining the two pos_enc along dim 2\n",
        "\n",
        "stacked = torch.stack([even_PE, odd_PE], dim = 2)\n",
        "stacked.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOm-xurEImn7",
        "outputId": "22114896-15c6-427f-91cb-82d41c9c5673"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concat the last two dimension\n",
        "\n",
        "PE = torch.flatten(stacked, start_dim = 1, end_dim = 2)\n",
        "PE.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tfA24P7IfTs",
        "outputId": "d99421ce-9b68-43e6-d096-bf83e7c301b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SUMMARY**"
      ],
      "metadata": {
        "id": "Ymc9mVVkLB5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "  def forward(self, x):\n",
        "    even_i = torch.arange(0, self.d_model, 2).float()\n",
        "    denominator = pow(10000, even_i / self.d_model)\n",
        "\n",
        "    assert x < self.max_seq_len, \"Max sequence length is {}\".format(self.max_seq_len)\n",
        "    position = torch.arange(x, dtype =torch.float).reshape(x, 1)\n",
        "\n",
        "    even_PE = torch.sin(position / denominator)\n",
        "    odd_PE = torch.cos(position / denominator)\n",
        "\n",
        "    stacked = torch.stack([even_PE, odd_PE], dim = 2)\n",
        "    PE = torch.flatten(stacked, start_dim = 1, end_dim = 2)\n",
        "\n",
        "    return PE"
      ],
      "metadata": {
        "id": "LEfBMG7BLFMe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PositionalEncoding(6, 20)\n",
        "pos_enc = pe.forward(10)\n",
        "pos_enc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSRVVy9EMLK_",
        "outputId": "3bbe1d62-6237-4a4f-f3a0-a02a5f3b7580"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_enc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dpJrB21OkU4",
        "outputId": "1af54c8b-cfe2-43a6-a46a-d3a8ebe3204b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
              "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
              "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
              "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
              "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
              "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
              "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
              "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
              "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}